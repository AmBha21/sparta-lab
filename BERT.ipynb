{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmBha21/sparta-lab/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9c0piXFuPAk",
        "outputId": "9001e376-2f07-4d4d-e9d0-e6c6c30c7c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kugoxBIKuzOX",
        "outputId": "14916953-7fd3-458b-c1ed-3966277b868b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# load BERT large cased model from HuggingFace\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test sentences with [MASK]\n",
        "test_sentences = [\n",
        "    \"In its economic policy, government should [MASK] taxes.\",\n",
        "    \"[MASK] is the biggest issue in the US that needs immediate attention.\",\n",
        "    \"In regards with immigration, government should [MASK] borders.\",\n",
        "    \"Abortion should be [MASK]\"\n",
        "]\n",
        "\n",
        "# Assuming device is defined as 'cuda' or 'cpu'\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load BERT large cased model from HuggingFace\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "\n",
        "# Initialize an empty list to hold all sentences' predictions\n",
        "all_predictions = []\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
        "    mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_logits = model(inputs).logits\n",
        "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "    # Apply softmax to convert logits to probabilities for all tokens\n",
        "    softmax_probabilities = F.softmax(mask_token_logits, dim=1).squeeze()\n",
        "\n",
        "    # Get the top 5 tokens and their indices based on the softmax probabilities\n",
        "    top_5_probs, top_5_indices = torch.topk(softmax_probabilities, 5)\n",
        "\n",
        "    sentence_predictions = []  # Initialize a list for the current sentence's predictions\n",
        "    for index, token_id in enumerate(top_5_indices):\n",
        "        word = tokenizer.decode([token_id])\n",
        "        probability = top_5_probs[index].item() * 100  # Convert to percentage\n",
        "        sentence_predictions.append((word, probability))  # Append the word and its probability as a tuple\n",
        "\n",
        "    all_predictions.append(sentence_predictions)  # Append the list of tuples to the main list\n",
        "\n",
        "# Now `all_predictions` contains the requested structure\n",
        "print(all_predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaLZQizggo26",
        "outputId": "b6144e58-fac0-44bd-af57-81ef1c4bd46a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('reduce', 19.900068640708923), ('raise', 9.521841257810593), ('lower', 8.05118903517723), ('avoid', 6.900997459888458), ('levy', 6.001708284020424)], [('this', 35.91068685054779), ('it', 21.119381487369537), ('that', 4.928801953792572), ('aids', 1.2806895188987255), ('abortion', 1.0910799726843834)], [('establish', 9.790080785751343), ('cross', 6.809724867343903), ('respect', 6.733939051628113), ('maintain', 5.1498424261808395), ('check', 5.070238560438156)], [('.', 77.30709314346313), (';', 19.100698828697205), ('!', 2.3338330909609795), ('?', 0.6443031132221222), ('...', 0.1718519371934235)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all other necessary imports and model, tokenizer initialization\n",
        "\n",
        "tolerance = 1e-5  # Define a tolerance for comparing floating-point numbers\n",
        "\n",
        "for idx, sentence in enumerate(test_sentences):\n",
        "    print(f\"Original Sentence: {sentence}\")\n",
        "\n",
        "    for word, original_probability in all_predictions[idx]:\n",
        "        target_token_id = tokenizer.convert_tokens_to_ids(word)\n",
        "\n",
        "        inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "        inputs = inputs.to(device)\n",
        "        mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            token_logits = model(inputs).logits\n",
        "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "        # Apply softmax to get probabilities for all tokens\n",
        "        all_probabilities = F.softmax(mask_token_logits, dim=1).squeeze()\n",
        "\n",
        "        # Get the probability of the target word\n",
        "        target_probability = all_probabilities[target_token_id].item()\n",
        "\n",
        "        # Check if the probabilities are approximately equal\n",
        "        are_probabilities_equal = abs(original_probability - target_probability * 100) < tolerance\n",
        "\n",
        "        print(f\"Probability of '{word}': {target_probability*100:.2f}%, Original: {original_probability:.2f}%\")\n",
        "        print(f\"Are the probabilities equal? {'Yes' if are_probabilities_equal else 'No'}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az8kqds0g-04",
        "outputId": "df6c13f9-fa1b-4c78-ee41-e618981c54eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: In its economic policy, government should [MASK] taxes.\n",
            "Probability of 'reduce': 19.90%, Original: 19.90%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'raise': 9.52%, Original: 9.52%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'lower': 8.05%, Original: 8.05%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'avoid': 6.90%, Original: 6.90%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'levy': 6.00%, Original: 6.00%\n",
            "Are the probabilities equal? Yes\n",
            "\n",
            "\n",
            "Original Sentence: [MASK] is the biggest issue in the US that needs immediate attention.\n",
            "Probability of 'this': 35.91%, Original: 35.91%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'it': 21.12%, Original: 21.12%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'that': 4.93%, Original: 4.93%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'aids': 1.28%, Original: 1.28%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'abortion': 1.09%, Original: 1.09%\n",
            "Are the probabilities equal? Yes\n",
            "\n",
            "\n",
            "Original Sentence: In regards with immigration, government should [MASK] borders.\n",
            "Probability of 'establish': 9.79%, Original: 9.79%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'cross': 6.81%, Original: 6.81%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'respect': 6.73%, Original: 6.73%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'maintain': 5.15%, Original: 5.15%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of 'check': 5.07%, Original: 5.07%\n",
            "Are the probabilities equal? Yes\n",
            "\n",
            "\n",
            "Original Sentence: Abortion should be [MASK]\n",
            "Probability of '.': 77.31%, Original: 77.31%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of ';': 19.10%, Original: 19.10%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of '!': 2.33%, Original: 2.33%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of '?': 0.64%, Original: 0.64%\n",
            "Are the probabilities equal? Yes\n",
            "Probability of '...': 0.17%, Original: 0.17%\n",
            "Are the probabilities equal? Yes\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dC07pPndib8"
      },
      "source": [
        "Setting left wing text variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzcPlcb4iETU"
      },
      "outputs": [],
      "source": [
        "left_wing_path = '/content/drive/My Drive/sparta lab/transcripts/left_videos.txt'\n",
        "\n",
        "with open(left_wing_path, \"r\") as fp:\n",
        "    lines = fp.readlines()  # Read all lines into a list\n",
        "\n",
        "portion_length = len(lines) // 100  # Calculate 1/8th of the total number of lines\n",
        "partial_text = lines[:portion_length]  # Extract the first 1/8th portion\n",
        "\n",
        "# Split the partial text into individual lines/sentences\n",
        "text = [line.strip() for line in partial_text if line.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwQ-Yv1fiUqN",
        "outputId": "28ecb06c-f586-408a-8744-a4d80454bd98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['he',\n",
              " 'checked off his equipment',\n",
              " 'hello my name is Abigail',\n",
              " 'over the last few years I have emailed',\n",
              " \"Britain's National healthcare provider\",\n",
              " \"133 times trying to get a doctor's\",\n",
              " 'appointment',\n",
              " 'this video is about what that was like',\n",
              " 'why big institutions fall apart and why',\n",
              " \"they're so difficult to change\"]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdiORnaRic_n"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byOZv36tu-0o"
      },
      "outputs": [],
      "source": [
        "#inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5fqWLnWvBTe"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ2VXgYcvDVq"
      },
      "outputs": [],
      "source": [
        "#inputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HWh5emCvG7o"
      },
      "outputs": [],
      "source": [
        "# create random array of floats with equal dimensions to input_ids tensor\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "# create mask array\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
        "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTa_pFrpvJL6",
        "outputId": "5ef64187-144a-4ee4-cb92-3501eb4f9eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[False, False, False,  ..., False, False, False],\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        ...,\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        [False,  True, False,  ..., False, False, False]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncidDAmlvMCc"
      },
      "outputs": [],
      "source": [
        "selection = []\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append(\n",
        "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtBMYMvMvOO1",
        "outputId": "c9db34fb-926f-4bb6-d636-996744d7ab86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[], [3], [], [8], []]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "selection[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSV-jfQfvRT8"
      },
      "outputs": [],
      "source": [
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = 103"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6c78K_dvToY"
      },
      "outputs": [],
      "source": [
        "#inputs.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTevk5C7vWxM"
      },
      "outputs": [],
      "source": [
        "class MeditationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pPtQXz7vYby"
      },
      "outputs": [],
      "source": [
        "dataset = MeditationsDataset(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEK5NAKqvZ9H"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZsnRBLkvc2w",
        "outputId": "cc0851d3-16a8-4847-bcd8-9219b1cd1a5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDL8gUniviax",
        "outputId": "77bc2adb-9a09-4150-f6f9-541eed634721"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# activate training mode\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VQ8m2kqO1S_W",
        "outputId": "e75666a5-8da3-4f3f-febe-f2a3b849750b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press Enter to continue...\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input(\"Press Enter to continue...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-MW27T2vms_",
        "outputId": "ab5cd2e0-8ac6-4475-cba1-c0cd63e661bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2084 [00:00<?, ?it/s]<ipython-input-15-6dccb434beb5>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Epoch 0: 100%|██████████| 2084/2084 [32:29<00:00,  1.07it/s, loss=0.000196]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "18WKudHUL6tRiOyfr6rGInLNuYLi50Mow",
      "authorship_tag": "ABX9TyNX2jsRiFBKJ1MZWwTZ3Ufo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}